<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Website for the Smooth Games Optimization and Machine Learning Workshop (NeurIPS2018)">
    <meta name="author" content="GauthierGidel (Github)">
    <title>Smooth Games Optimization and Machine Learning Workshop: Bridging Game Theory and Deep Learning</title>
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.css" rel="stylesheet">
    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet">
    <!-- Custom styles for this template -->
    <link href="css/grayscale.css" rel="stylesheet">
  </head>
  <body id="page-top">
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">SGO&ML Workshop</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <!-- <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#about">Home</a>
            </li> -->
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#CFC">Call for Contributions</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#Speakers">Invited Speakers</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#Schedule">Schedule</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#Papers">Papers</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#Committee">Organizers</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="index_2018.html">SGO-Workshop 2018
              </a>
            </li>
          </ul>
        </div>
      </div>
    </nav>
    <!-- Header -->
    <header class="masthead">
      <div class="container d-flex h-100 align-items-center">
        <div class="mx-auto text-center">
          <h3 class="text-white-50 mx-auto mt-2 mb-5">Smooth Games Optimization and Machine Learning Workshop:</h3>
          <h1 class="mx-auto my-0 text-uppercase">Bridging Game Theory and Deep Learning</h1><br><br><br>
          <h2 class="text-white-50 mx-auto mt-2 mb-5"><b><a href="https://nips.cc/Conferences/2019/Schedule?showEvent=13158"> West Exhibition Hall A, Sat Dec 14, 08:00 AM</a></b></h2>
          <h2 class="text-white-50 mx-auto mt-2 mb-5">Dec 14th, <a href="https://nips.cc/Conferences/2019" class="pagelink">NeurIPS2019</a>, Vancouver, </h2>
          <!-- <a href="#CFC" class="btn btn-primary js-scroll-trigger">Call for Contributions</a> -->
            <div class = "row">
          <h5 class="text-white-50 mx-auto mt-2 mb-5">
          <!-- We will provide the details of our call for contributions <a href="contributions.html">here</a> soon! <br> -->
          </h5>
            </div>
        </div>
      </div>
    </header>
    <!-- About Section -->
    <section id="about" class="about-section text-center">
      <div class="container">
        <h2 class="text-white mb-4">Overview</h2>
        <p class="text-white-75">
          Advances in generative modeling and adversarial learning have given rise to renewed interest in differentiable two-players games, with much of the attention falling on generative adversarial networks (GANs). Solving these games introduces distinct challenges compared to the standard minimization tasks that the machine learning (ML) community is used to. A symptom of this issue is ML and deep learning (DL) practitioners using optimization tools on game-theoretic problems. Recent work seeks to rectify this situation by bringing game theoretic tools into ML. At NeurIPS 2018 we held “Smooth games optimization in ML”, a workshop with this scope and goal in mind.

        <a href="https://sgo-workshop.github.io/index_2018.html">Last year’s workshop</a> addressed theoretical aspects of games in machine learning, their special dynamics, and typical challenges. Talks by Costis Daskalakis, Niao He, Jacob Abernethy and Paulina Grnarova emphasized various fundamental topics in a pure, simplified theoretical setting. A number of contributed talks and posters tackled similar questions. The workshop culminated in a <a href="https://www.youtube.com/watch?v=shyyPIPxwaM&feature=youtu.be&t=86">panel discussion</a> that identified a number of interesting questions.
        The aim of this workshop is to provide a platform for both theoretical and applied researchers from the ML, mathematical programming and game theory community to discuss the status of our understanding on the interplay between smooth games, their applications in ML, as well existing tools and methods for dealing with them. We are looking for contributions that identifies and discusses open, forward-looking problems of interest to the NeurIPS community. </p>
        
        <div class="row">
      </section>
      <!-- Projects Section -->
      <section id="Speakers" class="projects-section bg-light">
        <h2 class="text-black mb-4 text-center">Invited Speakers</h2>
        <div class="container">
          <!-- Featured Project Row -->
          <div class="row no-gutters mb-4 mb-lg-5" id="feifang">
            <div class="col-xl-3 col-lg-5">
              <img class="img-fluid mb-0 mb-lg-0" src="img/feifang.jpg" alt="">
            </div>
            <div class="col-xl-9 col-lg-7">
              <div class="featured-text text-center text-lg-left">
                <h4>
                <a href="https://feifang.info/" class="pagelink">
                   Fei Fang (CMU)
                </a></h4>
                <h5>Title:</h5>
                <p class="text-black-50 mb-0">
                   Integrating Machine Learning with Game Theory for Societal Challenges<br>
                </p>
                <br>
                <h5>Abstract:</h5>
                <p class="text-black-50 mb-0 small">
                  Real-world problems such as protecting critical infrastructure and cyber networks and protecting wildlife, fishery, and forest often involve multiple decision-makers. While game theory is an established paradigm for such problems, its applicability in practice is often limited by computational intractability in large games, the unavailability of game parameters and the lack of rationality of human players. On the other hand, machine learning has led to huge successes in various domains and can be leveraged to overcome the limitations of the game-theoretic analysis. In this talk, I will introduce our work on integrating machine learning with computational game theory for addressing societal challenges such as security and sustainability, covering the following directions: data-based game-theoretic reasoning, learning-powered strategy computation in large scale games, and end-to-end learning of game parameters.
                </p>
                <br>
                <h5>Short Bio:</h5>
                <p class="text-black-50 mb-0 small">
                Fei Fang is an Assistant Professor at the Institute for Software Research in the School of Computer Science at Carnegie Mellon University. Before joining CMU, she was a Postdoctoral Fellow at the Center for Research on Computation and Society (CRCS) at Harvard University. She received her Ph.D. from the Department of Computer Science at the University of Southern California in June 2016.</p>
              </div>
            </div>
          </div>
          <div class="row no-gutters mb-4 mb-lg-5" id="eva">
            <div class="col-xl-3 col-lg-2">
              <img class="img-fluid mb-3 mb-lg-0" src="img/eva_tar.jpg" alt="">
            </div>
            <div class="col-xl-9 col-lg-10">
              <div class="featured-text text-center text-lg-left">
                <h4>
                <a href="https://www.engineering.cornell.edu/faculty-directory/eva-tardos" class="pagelink">
                   Eva Tardos (Cornell University)
                </a></h4>
                <h5>Title:</h5>
                <p class="text-black-50 mb-0">
                 Learning in dynamic multi-agent environments
                </p>
                <br>
                <h5>Abstract:</h5>
                <p class="text-black-50 mb-0 small">
                 In this talk we will consider on games where players use a form of learning that helps them adapt to a changing environment. We ask if the quantitative guarantees obtained for Nash equilibria for this class of games extend to such out of equilibrium game play, when the game or the population of players is dynamically changing and where participants have to adapt to the dynamic environment.
                </p>
                <br>
                <h5>Short Bio:</h5>
                <p class="text-black-50 mb-0 small">
                 Éva Tardos received her Dipl.Math. in 1981 , and her Ph.D. 1984, from Eötvös University , Budapest, Hungary . She joined Cornell in 1989, and was Chair of the Department of Computer Science 2006-2010. She has been elected to the National Academy of Engineering, National Academy of Sciences, and the American Academy of Arts and Sciences, is an external member of the Hungarian Academy of Sciences, and is the recipient of a number of fellowships and awards including the the IEEE John von Neumann Medal, Packard Fellowship, the Gödel Prize, Dantzig Prize, and the Fulkerson Prize. She was editor editor-in-Chief of SIAM Journal of Computing 2004-2009, and is currently editor-in-Chief of the Journal of the ACM, and editor of some other journals includingthe Theory of Computing, and Combinatorica.
                </p>
              </div>
            </div>
          </div>
          <div class="row no-gutters mb-4 mb-lg-5" id="david">
            <div class="col-xl-3 col-lg-5">
              <img class="img-fluid mb-3 mb-lg-0" src="img/david.png" alt="">
            </div>
            <div class="col-xl-9 col-lg-7">
              <div class="featured-text text-center text-lg-left">
                <h4>
                <a href="https://sites.google.com/site/dbalduzzi/" class="pagelink">
                 David Balduzzi (DeepMind)
                </a></h4>
                <h5>Title:</h5>
                <p class="text-black-50 mb-0">
                Composition, learning, and games
                </p>
                <br>
                <h5>Abstract:</h5>
                <p class="text-black-50 mb-0 small">
                 Gradient descent and automatic differentiation provide a powerful framework for composing function-approximators and training them to optimise an objective. However, there are many learning algorithms that do not optimise a single, fixed objective — such as self-play and its generalisations in Go and StarCraft, generative adversarial networks, and adversarial training for robustness.

                 In this talk, I will argue for a new subfield of “differentiable mechanism design”. In support, I will describe extant work from the literature under the unifying themes of meta-games and second-order information.
                </p>
                <br>
                <h5>Short Bio:</h5>
                <p class="text-black-50 mb-0 small">
                  David Balduzzi is a researcher at Google DeepMind. He did his PhD in representation theory and algebraic geometry at the University of Chicago. After that he worked on computational neuroscience at UW-Madison and machine learning at the MPI for Intelligent Systems, ETH Zürich and Victoria University Wellington. He now works on game theory and machine learning at DeepMind.

                </p>
              </div>
            </div>
          </div>
          <div class="row no-gutters mb-4 mb-lg-5" id="Aryan">
            <div class="col-xl-3 col-lg-3">
              <img class="img-fluid mb-3 mb-lg-0" src="img/Aryan.jpg" alt="">
            </div>
            <div class="col-xl-9 col-lg-9">
              <div class="featured-text text-center text-lg-left">
                <h4>
                <a href="http://sites.utexas.edu/mokhtari/" class="pagelink">
                  Aryan Mokhtari (UT Austin)
                </a></h4>
                <h5>Title:</h5>
                <p class="text-black-50 mb-0">
                  Understanding the Role of Optimism in Minimax Optimization: A Proximal Point Approach
                </p>
                <br>
                <h5>Abstract:</h5>
                <p class="text-black-50 mb-0 small">
                  In this talk, we consider solving saddle point problems, and, in particular, we discuss the concept of “optimism” or “negative momentum” - a technique which is observed to have superior empirical performance in training GANs. The goal of this talk is to provide a theoretical understanding on why optimism helps, in particular why the Optimistic Gradient Descent Ascent (OGDA) algorithm performs well in practice. To do so, we first consider the classical Proximal Point algorithm which is an implicit algorithm to solve this problem. We then show that OGDA inherently tries to approximate the proximal point method, and this is the rationale behind the ‘’negative momentum” term in the update of OGDA. This proximal point approximation viewpoint also enables us to provide a much simpler analysis of another well studied algorithm - the Extra-Gradient (EG) method.
                </p>
                <br>
                <h5>Short Bio:</h5>
                <p class="text-black-50 mb-0 small">
                  Aryan Mokhtari is an Assistant Professor in the ECE Department of the University of Texas at Austin (UT Austin). Before joining UT Austin, he was a Postdoctoral Associate in the Laboratory for Information and Decision Systems (LIDS) at MIT. Before that, he was a Research Fellow at the Simons Institute for the Theory of Computing at UC Berkeley. His research interests include the areas of optimization, machine learning, and artificial intelligence. His current research focuses on the theory and applications of convex and non-convex optimization in large-scale machine learning and data science problems.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>

       <section id="Schedule" class="about-section text-center">
      <div class="container">
        <div class="row">
          <!-- Project Two Row -->
          <div id="Schedule" class="row justify-content-center no-gutters">
            <div class="col-lg-12">
              <div class="bg-dark text-center h-100 project">
                <div class="d-flex h-100">
                  <div class="project-text w-100 my-auto text-center text-lg-left">
                    <h4 class="text-white" align="left">Morning Schedule</h4>
                    <table class="table table-dark table-striped">
                      <thead>
                        <tr>
                          <th scope="col">Time</th>
                          <th scope="col">Speaker</th>
                          <th scope="col">Title</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <th scope="row">8:15</th>
                          <td>Ioannis Mitliagkas</td>
                          <td>Opening remarks</td>
                        </tr>
                        <tr>
                          <th scope="row">8:30</th>
                          <td>
                            Invited talk, <a href="#eva">Eva Tardos</a></td>
                          <td>
                            Learning in dynamic multi-agent environments <a href="#eva">[abstract]</a></td>
                        </tr>
                        <tr>
                          <th scope="row">9:10</th>
                          <td>Poster Spotlights:<hr>
                          David Fridovich-Keil <hr>
                          Eric Mazumdar <hr>
                          Olya Ohrimenko <hr>
                          Yan Yan<hr>
                          Guojun Zhang <hr>
                          Shuang Li <hr>
                          Shuang Li <hr>
                          Kevin Lai <hr>
                          Mingrui Liu† <hr>
                          Lisa Lee <hr>
                          </td>
                          <td>
                            -
                          <hr>
                          <a href="CameraReady2019/20.pdf">Stable, Efficient Solutions for Differential Games with Feedback Linearizable Dynamics</a>
                          <hr>
                          <a href="CameraReady2019/28.pdf">
                          Policy Gradient in Linear Quadratic Dynamic Games Has No Convergence Guarantees 
                          </a>
                          <hr>
                            <a href="CameraReady2019/6.pdf">
                          Collaborative Machine Learning Markets
                            </a>
                           <hr>
                            <a href="CameraReady2019/10.pdf">
                          Sharp Analysis of Simple Restarted Stochastic Gradient for Min-Max Optimization
                            </a>
                          <hr>
                            <a href="CameraReady2019/24.pdf">
                          Convergence Behaviour of Some Gradient-Based Methods on Bilinear Zero-Sum Games 
                            </a><hr>
                          <a href="CameraReady2019/21.pdf">
                           Cubic Regularization for Differentiable Games 
                          </a><hr>
                          <a href="CameraReady2019/18.pdf">
                          Geometry Correspondence between Empirical and Population Games  
                          </a><hr>
                          <a href="CameraReady2019/14.pdf">
                            Last-iterate convergence rates for min-max optimization
                          </a><hr>
                          <a href="CameraReady2019/15.pdf">
                          Decentralized Parallel Algorithm for Training Generative Adversarial Nets 
                          </a><hr>
                          <a href="https://arxiv.org/pdf/1906.05274.pdf">
                          Efficient Exploration via State Marginal Matching
                          </a><hr>
                          </td>
                        </tr>
                        <tr>
                          <th scope="row">9:30</th>
                          <td>Poster session + Coffee break</td>
                          <td></td>
                        </tr>
                          <th scope="row">11:00</th>
                          <td>
                            Invited talk, <a href="#david">: David Balduzzi </a></td>
                          <td>
                            Composition, learning, and games <a href="#david">[abstract]</a></td>
                        </tr>
                        <tr>
                          <th scope="row">11:40</th>
                          <td>Contributed Talk, Praneeth Netrapalli
                          </td>
                          <td>
                           What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?</td>
                        </tr>
                        <tr>
                          <th scope="row">12:05</th>
                          <td>Contributed talk, Tanner Fiez
                          </td>
                          <td>
                           Characterizing Equilibria in Stackelberg Games</td>
                        </tr>
                        <tr>
                          <th scope="row">12:30</th>
                          <td>Lunch break</td>
                          <td></td>
                        </tr>
                      </tbody>
                    </table>
                    <hr class="d-none d-lg-block mb-0 mr-0">
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-12">
              <div class="bg-grey text-center h-100 project">
                <div class="d-flex h-100">
                  <div class="project-text w-100 my-auto text-center text-lg-left">
                    <br>
                    <hr>
                    <h4 class="text-white" align="left">Afternoon Schedule</h4>
                    <table class="table table-dark table-striped">
                      <thead>
                        <tr>
                          <th scope="col">Time</th>
                          <th scope="col">Speaker</th>
                          <th scope="col">Title</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <th scope="row">14:00</th>
                          <td> Invited talk,
                            <a href="#feifang">Fei Fang</a></td>
                          <td>
                            Integrating Machine Learning with Game Theory for Societal Challenges
                            <br><a href="#feifang">[abstract]</a></td>
                        </tr>
                        <tr>
                          <th scope="row">14:40</th>
                          <td>Contributed talk,
                          Yuanhao Wang
                          </td>
                          <td> On Solving Local Minimax Optimization: A Follow-the-Ridge Approach</td>
                        </tr>
                        <tr>
                          <th scope="row">15:05</th>
                          <td>Contributed talk,
                          Elizabeth Bondi
                          </td>
                          <td> Exploiting Uncertain Real-Time Information from Deep Learning in Signaling Games for Security and Sustainability
                          </td>
                        </tr>
                        <tr>
                          <th scope="row">15:30</th>
                          <td>Coffee break</td>
                          <td></td>
                        </tr>
                        <tr>
                          <th scope="row">16:00</th>
                          <td> Invited talk,
                            <a href="#Aryan">Aryan Mokhtari</a></td>
                          <td>
                            Understanding the role of optimism in minimax optimization<br>
                          <a href="#Aryan">[abstract]</a></td>
                        </tr>
                        <tr>
                          <th scope="row">16:40</th>
                          <td>Poster spotlights:<hr>
                          Andrew Bennett<hr>
                          Moksh Jain<hr>
                          Ryan D'Orazio<hr>
                          Benjamin Chasnov<hr>
                          Hongkai Zheng<hr>
                          Christos Tsirigotis<hr>
                          Ian Gemp<hr>
                          Konstantin Mishchenko <hr>
                          Gabriele Farina<hr>
                          Adam Lerer<hr>
                          Ioannis Panageas
                          </td>
                          <td> - <hr>
                          <a href="CameraReady2019/33.pdf">
                          Deep Generalized Method of Moments for Instrumental Variable Analysis
                          </a><hr>
                          <a href="CameraReady2019/9.pdf">
                          Proximal Policy Optimization for Improved Convergence in IRGAN  
                          </a><hr>
                            <a href="CameraReady2019/34.pdf">
                          Bounds for Approximate Regret-Matching Algorithms
                            </a><hr>
                            <a href="CameraReady2019/31.pdf">
                          Opponent Anticipation via Conjectural Variations
                            </a><hr>
                            <a href="CameraReady2019/37.pdf">
                          Implicit competitive regularization in GANs 
                            </a><hr>
                            <a href="CameraReady2019/23.pdf">
                            Objectives Towards Stable Adversarial Training Without Gradient Penalties
                            </a><hr>
                            <a href="CameraReady2019/11.pdf">
                            The Unreasonable Effectiveness of Adam on Cycles
                            </a><hr>
                            <a href="CameraReady2019/12.pdf">
                            Revisiting Stochastic Extragradient 
                            </a><hr>
                            <a href="CameraReady2019/35.pdf">
                            Compositional Calculus of Regret Minimizers 
                            </a><hr>
                            <a href="https://arxiv.org/pdf/1912.02318.pdf">
                            Search in Cooperative Partially Observable Games
                            </a><hr>
                            <a href="https://arxiv.org/pdf/1807.04252.pdf">
                            Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization
                            </a>
                          </td>
                        </tr>
                        <tr>
                          <th scope="row">17:00</th>
                          <td>Discussion panel</td>
                          <td>
                          David Balduzzi, 
                          Elizabeth Bondi, 
                          Noam Brown, 
                          Praneeth Netrapalli, 
                          Eva Tardos, 
                          Jakob Foerster</td>
                        </tr>
                        <tr>
                          <th scope="row">17:30</th>
                          <td>Organizers</td>
                          <td>  Concluding remarks -- afternoon poster session</td>
                        </tr>
                        <tr>
                          <th scope="row">18:30</th>
                          <td>Workshop ends</td>
                          <td></td>
                        </tr>
                      </tbody>
                    </table>
                    <hr class="d-none d-lg-block mb-0 mr-0">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>

      <section id="Papers" class="about-section text-center">
      <div class="container">
        <div class="row">
            <div class="col-lg-10">
              <div class="text-center h-100 project">
                <div class="d-flex h-100">
                  <div class="project-text w-100 my-auto text-center text-lg-left">
                    <h4 class="text-white">Accepted Contributions</h4>
                    <p class="mb-0 text-white-50">
                    <hr class="d-none d-lg-block mb-0 ml-0">
                    <div class="row no-gutters mb-4 mb-lg-5">
              <ul class="text-white-75 text-justify">
                    <li>
                      <a href="CameraReady2019/4.pdf">
                    Learning Maximin Strategies in Simulation-Based Games with Infinite Strategy Spaces Marchesi, Alberto*; Trovò, Francesco; Gatti, Nicola
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/5.pdf">
                    Learning to Correlate in Multi-Player General-Sum Sequential Games  Celli, Andrea; 
                    Marchesi, Alberto*; Bianchi, Tommaso; Gatti, Nicola
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/6.pdf">
                    Collaborative Machine Learning Markets  Ohrimenko, Olga*; Tople, Shruti; Tschiatschek, 
                    Sebastian
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/7.pdf">
                    A Closer Look at the Optimization Landscapes ofGenerative Adversarial Networks
                    Berard, Hugo*; Gidel, Gauthier; Almahairi, Amjad; Vincent, Pascal; Lacoste-Julien, Simon
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/8.pdf">
                    Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization 
                    Daskalakis, Constantinos; Panageas, Ioannis*
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/9.pdf">
                    Proximal Policy Optimization for Improved Convergence in IRGAN  
                    Jain, Moksh M*; Kamath S, Sowmya
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/10.pdf">
                    Sharp Analysis of Simple Restarted Stochastic Gradient Methods for Min-Max Optimization
                    Yan, Yan*; Xu, Yi; Lin, Qihang; Liu, Wei; Yang, Tianbao
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/11.pdf">
                    The Unreasonable Effectiveness of Adam on Cycles  McWilliams, Brian; Gemp, Ian*
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/17.pdf">
                    Learning Adversarial Interactions in Stackelberg Security Games with Limited Data 
                    Fang, Boli*; Jiang, Miao; Tregubov, Alexey; Blythe, James ; Ferrara, Emilio
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/13.pdf">
                    Agent Robustness to Negative Interruptions: A Multi-agent Learning Approach Çelikok, 
                    Mustafa Mert*; Peltola, Tomi; Kaski, Samuel
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/15.pdf">
                    Decentralized Parallel Algorithm for Training Generative Adversarial Nets 
                    Liu, Mingrui*; Mroueh, Youssef; Zhang, Wei; Cui, Xiaodong; Yang, Tianbao; Das, Payel
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/18.pdf">
                    Geometry Correspondence between Empirical and Population Games  
                    Li, Shuang*; Li, Qiuwei; Tang, Gongguo; Wakin, Michael
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/20.pdf">
                    Stable, Efficient Solutions for Differential Games with Feedback Linearizable Dynamics
                    Fridovich-Keil, David; Rubies Royo, Vicenç; Tomlin, Claire
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/21.pdf">
                    Cubic Regularization for Differentiable Games 
                    Li, Shuang*; Xie, Youye; Li, Qiuwei; Tang, Gongguo
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/14.pdf">
                        Last-iterate convergence rates for min-max optimization Abernethy, Jacob D; Lai, Kevin A*; Wibisono, Andre
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/22.pdf">
                    On Solving Local Minimax Optimization: A Follow-the-Ridge Approach  Wang, Yuanhao*; 
                    Zhang, Guodong; Ba, Jimmy; Grosse, Roger B
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/16.pdf">
                    Extra-gradient with player sampling for provable fast convergence in n-player games 
                    Domingo-Enrich, Carles*; Jelassi, Samy; Scieur, Damien; Mensch, Arthur; Bruna, Joan
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/23.pdf">
                    Objectives Towards Stable Adversarial Training Without Gradient Penalties
                    Tsirigotis, Christos*; Hjelm, Devon; Courville, Aaron; Mitkas, Pericles
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/24.pdf">
                    Convergence Behaviour of Some Gradient-Based Methods on Bilinear Zero-Sum Games 
                    Zhang, Guojun*; Yu, Yaoliang
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/25.pdf">
                    Linear Lower Bounds and Conditioning of Differentiable Games 
                    Ibrahim, Adam*; Azizian, Waïss; Gidel, Gauthier; Mitliagkas, Ioannis
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/26.pdf">
                    Search in Cooperative Partially Observable Games  
                    Lerer, Adam; Hu, Hengyuan; Foerster, Jakob; Brown, Noam*
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/27.pdf">
                    Exploiting Uncertain Real-Time Information from Deep Learning in Signaling Games for 
                    Security and Sustainability 
                    Bondi, Elizabeth*; Oh, Hoon; Xu, Haifeng; Fang, Fei; Dilkina, Bistra; Tambe, Milind
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/29.pdf">
                    What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?  
                    Jin, Chi*; Netrapalli, Praneeth; Jordan, Michael
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/30.pdf">
                    Characterizing Equilibria in Stackelberg Games  
                    Fiez, Tanner*; Chasnov, Benjamin; Ratliff, Lillian 
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/31.pdf">
                    Opponent Anticipation via Conjectural Variations  
                    Chasnov, Benjamin*; Fiez, Tanner; Ratliff, Lillian 
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/28.pdf">
                    Policy Gradient in Linear Quadratic Dynamic Games Has No Convergence Guarantees 
                    Mazumdar, Eric V*; Ratliff, Lillian ; Jordan, Michael; Sastry, Shankar
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/33.pdf">
                    Deep Generalized Method of Moments for Instrumental Variable Analysis 
                    Bennett, Andrew*; Kallus, Nathan; Schnabel, Tobias
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/34.pdf">
                    Bounds for Approximate Regret-Matching Algorithms 
                    D'Orazio, Ryan*; Morrill, Dustin; Wright, James R
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/35.pdf">
                    Compositional Calculus of Regret Minimizers 
                    Farina, Gabriele*; Kroer, Christian; Sandholm, Tuomas
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/36.pdf">
                        Variance Reduction for Matrix Games Carmon,
                    Yair*; Jin, Yujia; Sidford, Aaron; Tian, Kevin
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/37.pdf">
                    Implicit competitive regularization in GANs 
                    Schaefer, Florian T*; Zheng, Hongkai; Anandkumar, Animashree
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/12.pdf">
                    Revisiting Stochastic Extragradient 
                    Mishchenko, Konstantin*; Kovalev, Dmitry; Richtarik, Peter; Malitsky, Yura
                      </a>
                    </li>

                    <li>
                      <a href="CameraReady2019/20.pdf">
                    Stable, Efficient Solutions for Differential Games with Feedback Linearizable Dynamic
                    Fridovich-Keil, David*; Rubies Royo, Vicenç; Tomlin, Claire
                      </a>
                    </li>

                  </div>
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>
      <!-- CFP Section -->
      <section id="CFC" class="about-section text-center">
        <div class="container">
          <div class="row">
            <div class="col-lg-8 mx-auto">
              <h4 class="text-white-75"> Call for Contributions</h4>
              <p class="text-white-75">We are soliciting contributions that address one of the below questions, or secondarily, another question on the intersection of modern machine learning and games. This year we are particularly interested in accepting work that uses non-standard formulations and applications for games in ML. 

              <ul class="text-white-75 text-justify">
              <li>How can we integrate learning with game theory? (e.g. [Schuurmans et al., 2016]) </li>
              <li>How can we inject deep learning into games and vice-versa (eg.  actor-critic formulations can be cast as a game)?</li>
              <li>What are the practical implications and applications? </li>
              <li>How do we go beyond the standard GAN discussion and model general agents that interact with each other in a learning context?</li>
              <li>What can we say about the existence and uniqueness results of equilibria in smooth games?</li>
              <li>Can we approximate mixed equilibria have better properties than the exact ones? [Arora, S., Ge, R., Liang, Y., Ma, T., Zhang, 2017]  [Lipton et al., 2002]</li>
              <li>Can we define a weaker notion of  solution than Nash Equilibria? [Papadimitriou, Piliouras, 2018]</li>
              <li>Can we compare the quality/performance of Nash equilibria/cycles ? Are there points that have a better quality/outcome than Nash equilibria ? [Kleinberg et al. 2011]</li>
              <li>How do we design efficient algorithms that are guaranteed to achieve the desired solutions? </li>
              <li>Finally, how do we design better objectives to match a specific ML task at hand?</li>
            </p>
              <h4 class="text-white-75"> Submission details </h4>
              <p class="text-white-75 text-lg-left">
                A submission should take the form of an <b>anonymous</b> extended abstract (2-4 pages long <b>excluding references</b>) in PDF format using the following <a href="template/neurips_2019.sty">modified NeurIPS style</a>. The submission process will be handled via
                <a href= "https://cmt3.research.microsoft.com/SGOML2019">CMT</a>.
                Previously published work (or under-review) is acceptable, though it needs to be clearly indicated as published work when submitting. Please provide as a footnote in the actual pdf indicating the venue where the work has been submitted.
                Submissions can be accepted as contributed talks, spotlight or poster presentations (all accepted submissions can have a poster). Extended abstracts must be submitted by September 16, 2019 (11:59pm AoE). Final versions will be posted on the workshop website (and are archival but <b>do not constitute a proceedings</b>).
              </p>
              <p>
                A limited number of NeurIPS registration slots will be available for accepted talks and posters to this workshop. We do not have control over the number, so it might happen that not all accepted posters get a slot. We strongly advise you to first try to register through the NeurIPS lottery to increase your chances of getting a registration slot. If you get a registration slot it means that you are guaranteed the ability to register, but you will have to pay for the registration.
              </p>
              <h2 class="text-white mb-4">Key Dates:</h2>
              <ul class="list-group">
                <li class="list-group-item list-group-item-dark">Abstract submission deadline: September 16, 2019 (11:59pm AoE) via <a href= "https://cmt3.research.microsoft.com/SGOML2019">CMT</a></li>
                <li class="list-group-item list-group-item-dark">Acceptance notification: October 1, 2019</li>
              </ul>
            </div>
          </div>
        </div>
      </section>
      
      <section id="Committee" class="projects-section bg-light">
        <h2 class="text-black mb-4 text-center">Organizers</h2>
        <div class="container">
            <div class="row no-gutters mb-4 mb-lg-5">
              <div class="col-xl-3 col-lg-3">
                <img class="img-fluid mb-3 mb-lg-0" src="img/yannis.jpg" alt="">
              </div>
              <div class="col-xl-8 col-lg-8">
                <div class="featured-text text-center text-lg-left">
                  <h4>
                  <a href="http://mitliagkas.github.io" class="pagelink">
                    Ioannis Mitliagkas (Mila & University of Montreal)
                  </a></h4>
                  <p class="text-black-50 mb-0">
                    Ioannis Mitliagkas is an assistant professor in the department of Computer Science and Operations Research (DIRO) at the University of Montréal. Before that, he was a Postdoctoral Scholar with the departments of Statistics and Computer Science at Stanford University. He obtained his Ph.D. from the department of Electrical and Computer Engineering at The University of Texas at Austin. His research includes topics in optimization, statistical learning and inference, and efficient large-scale and distributed algorithms.
                    <br>
                    He is particularly interested in the dynamics of optimization, like momentum methods, in the presence of system dynamics, adaptivity, and lately, smooth two-player games (ongoing work).
                  </p>
                </div>
              </div>
            </div>
            <div class="row no-gutters mb-4 mb-lg-5">
              <div class="col-xl-3 col-lg-3">
                <img class="img-fluid mb-3 mb-lg-0" src="img/gauthier.jpg" alt="">
              </div>
              <div class="col-xl-8 col-lg-8">
                <div class="featured-text text-center text-lg-left">
                  <h4>
                  <a href="http://gauthiergidel.github.io/" class="pagelink">
                    Gauthier Gidel (Mila & University of Montreal)
                  </a></h4>
                  <p class="text-black-50 mb-0">
                    Gauthier Gidel is a PhD candidate at Mila lab Université de Montréal under the supervision of Simon Lacoste-Julien. Before that, he received the Diplôme de l’École Normale Supérieure in 2017 (ULM MPI2013) and the Master of Science MVA from École Normale supérieur Paris-Saclay in 2016.
                    Gauthier’s PhD thesis topic revolves around saddle point optimization (a.k.a mini-max problems) for machine learning and more generally on understanding the practical and theoretical challenges of differentiable games optimization for multi-agent learning. He is also a recipient of a Graduate Borealis AI fellowship. 
                    <!-- <br> -->
                  </p>
                </div>
              </div>
            </div>
            <div class="row no-gutters mb-4 mb-lg-5">
              <div class="col-xl-3 col-lg-3">
                <img class="img-fluid mb-3 mb-lg-0" src="img/niao.jpg" alt="">
              </div>
              <div class="col-xl-8 col-lg-8">
                <div class="featured-text text-center text-lg-left">
                  <h4>
                  <a href="http://niaohe.ise.illinois.edu/" class="pagelink">
                  Niao He (UIUC)
                  </a></h4>
                  <p class="text-black-50 mb-0">
                    Niao He is an assistant professor in the Department of Industrial and Enterprise Systems Engineering and Coordinated Science Laboratory at the University of Illinois at Urbana-Champaign. Before joining Illinois, she received her Ph.D. degree in Operations Research from Georgia Institute of Technology in 2015 and B.S. degree in Mathematics from University of Science and Technology of China in 2010. Her research interests are in large-scale optimization and machine learning, with a primary focus in bridging modern optimization theory and algorithms with core machine learning topics, like Bayesian inference, reinforcement learning, and adversarial learning. She is also a recipient of the NSF CISE Research Initiation Initiative (CRII) Award and the NCSA Faculty Fellowship.
                  </p>
                </div>
              </div>
            </div>
            <div class="row no-gutters mb-4 mb-lg-5">
              <div class="col-xl-3 col-lg-3">
                <img class="img-fluid mb-3 mb-lg-0" src="img/reyhane.jpg" alt="">
              </div>
              <div class="col-xl-8 col-lg-8">
                <div class="featured-text text-center text-lg-left">
                  <h4>
                  <a href="https://reyhaneaskari.github.io/" class="pagelink">
                  Reyhane Askari (Mila & University of Montreal)
                  </a></h4>
                  <p class="text-black-50 mb-0">
                    Reyhane Askari is a PhD student at Mila lab, Université de Montréal. She works under the supervision of Ioannis Mitliagkas (UdeM) and Nicolas Le Roux (Google Brain). Prior to her PhD, she received her Masters in Computer Science from Université de Montréal and started working as a Machine Learning engineer for two years at Mila. During that time she worked on several open-source software for deep learning such as Theano, Orion and Cortex. She also did her bachelors in Computer Engineering at Amirkabir University of Technology (Tehran Polytechnic).
                    <br>
                    Her research interests are on understanding accelerated methods in single objective and multi-objective settings using tools from dynamical systems.
                  </p>
                </div>
              </div>
            </div>
            <div class="row no-gutters mb-4 mb-lg-5">
              <div class="col-xl-3 col-lg-3">
                <img class="img-fluid mb-3 mb-lg-0" src="img/nika.jpg" alt="">
              </div>
              <div class="col-xl-8 col-lg-8">
                <div class="featured-text text-center text-lg-left">
                  <h4>
                  <a href="https://www.cs.cornell.edu/~nika/" class="pagelink">
                  Nika Haghtalab
                  </a></h4>
                  <p class="text-black-50 mb-0">
                    Nika Haghtalab is an Assistant Professor in the Department of Computer Science at Cornell University. She works broadly on the theoretical aspects of machine learning and algorithmic economics. She especially cares about developing a theory for machine learning that accounts for its interactions with people and organizations, and the wide range of social and economic limitations, aspiration, and behavior they demonstrate. Prior to Cornell, she was a postdoctoral researcher at Microsoft Research, New England, in 2018-2019.
                    <br>
                    She received her Ph.D. from the Computer Science Department of Carnegie Mellon University, where she was co-advised by Avrim Blum and Ariel Procaccia. Her thesis titled Foundation of Machine Learning, by the People, for the People received the CMU School of Computer Science Dissertation Award (2018) and a SIGecom Dissertation Honorable Mention Award (2019).
                  </p>
                </div>
              </div>
            </div>
            <div class="row no-gutters mb-4 mb-lg-5">
                <div class="col-xl-3 col-lg-3">
                  <img class="img-fluid mb-3 mb-lg-0" src="img/SLJ.jpg" alt="">
                </div>
                <div class="col-xl-8 col-lg-8">
                  <div class="featured-text text-center text-lg-left">
                    <h4>
                    <a href="https://www.di.ens.fr/~slacoste/" class="pagelink">
                      Simon Lacoste-Julien (Mila & University of Montreal)
                  </a></h4>
                  <p class="text-black-50 mb-0">
                    Simon Lacoste-Julien is a CIFAR fellow and an assistant professor at Mila and DIRO from Université de Montréal. His research interests are machine learning and applied math, with applications to computer vision and natural language processing. He obtained a B.Sc. in math., physics and computer science from McGill, a PhD in computer science from UC Berkeley and a post-doc from the University of Cambridge. He spent a few years as a research faculty at INRIA and École normale supérieure in Paris before coming back to his roots in Montreal in 2016.
                    <br>
                    Simon published several papers at the intersection of mathematical programming and machine learning, and in particular for solving min-max games. He is a frequent participant to the NeurIPS OPT workshop series, and co-organized the NeurIPS 2009 workshop on <a href="www.gen-disc2009.wikidot.com"> “The Generative & Discriminative Learning Interface” </a>. </p>
                  </div>
                </div>
              </div>
          </section>

          <section id="Refs" class="projects-section bg-light">
            <h2 class="text-black mb-4 text-center">Relevant References</h2>
            <div class="container">
              <div class="row no-gutters mb-4 mb-lg-5">
                <p>Abernethy, J.D., Bartlett, P.L., Rakhlin, A., Tewari, A., Optimal strategies and minimax lower bounds for online convex games. In COLT 2009.</p>
                <p>Arora, S., Ge, R., Liang, Y., Ma, T., Zhang, Y., Generalization and Equilibrium in Generative Adversarial Nets (GANs). In ICML 2017.</p>
                <p> Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K. and Graepel, T., 2018. The Mechanics of n-Player Differentiable Games. In ICML 2018.</p>
                <p>Daskalakis, C., Goldberg, P., Papadimitriou, C., The Complexity of Computing a Nash Equilibrium. SIAM J. Comput., 2009.</p>
                <p>Daskalakis, C., Ilyas, A., Syrgkanis, V., Zeng, H., Training GANs with Optimism. In ICLR 2018.</p>
                <p>Ewerhart, C., Ordinal Potentials in Smooth Games (SSRN Scholarly Paper No. ID 3054604). Social Science Research Network, Rochester, NY, 2017.</p>
                <p>Fedus, W., Rosca, M., Lakshminarayaan, B., Dai, A.M., Mohamed, S., Goodfellow, I., Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step. In ICLR 2018.</p>
                <p>Gidel, G., Jebara, T., Lacoste-Julien, S. Frank-Wolfe Algorithms for Saddle Point Problems. In AISTATS 2017.</p>
                <p>Gidel, G., Berard,H., Vincent, P., Lacoste-Julien, S., A Variational Inequality Perspective on Generative Adversarial Networks. arXiv:1802.10551 [cs, math, stat], 2018.</p>
                <p>Grnarova, P., Levy, K.Y., Lucchi, A., Hofmann, T., Krause, A., An Online Learning Approach to Generative Adversarial Networks. In ICLR 2018.</p>
                <p>Harker, P.T., Pang, J.-S., Finite-dimensinal variational inequality and nonlinear complementarity problems: A survey of theory, algorithms and applications. Mathematical Programming, 1990.</p>
                <p>Hazan, E., Singh, K., Zhang, C., Efficient Regret Minimization in Non-Convex Games, in ICML 2017.</p>
                <p>Karlin, S., Weiss, G., The Theory of Infinite Games, Mathematical Methods and Theory in Games, Programming, and Economics, 1959.</p>
                <p>Lipton, R.J., Young, N.E., Simple Strategies for Large Zero-sum Games with Applications to Complexity Theory, in STOC 94.</p>
                <p>Mescheder, L., Nowozin, S., Geiger, A., The Numerics of GANs. In NeurIPS 2017.</p>
                <p>Nisan, N., Roughgarden, T., Tardos, E., Vazirani, V., Algorithmic Game Theory. Cambridge University Press, 2007.</p>
                <p>Pfau, D., Vinyals, O., Connecting Generative Adversarial Networks and Actor-Critic Methods. arXiv:1610.01945 [cs, stat], 2016.</p>
                <p>Roughgarden, T., Intrinsic Robustness of the Price of Anarchy, in: Communications of The ACM - CACM, 2009.</p>
                <p>Scutari, G., Palomar, .P., Facchinei, F., Pang, J. s., Convex Optimization, Game Theory, and Variational Inequality Theory. IEEE Signal Processing Magazine, 2010.</p>
                <p>Syrgkanis, V., Agarwal, A., Luo, H., Schapire, R.E., Fast Convergence of Regularized Learning in Games, in NeurIPS 2015.</p>
                <p>Von Neumann, J., Morgenstern, O., Theory of Games and Economic Behavior. Princeton University Press, 1944.</p>
                <p>Schuurmans, Dale, and Martin A. Zinkevich. "Deep learning games." Advances in Neural Information Processing Systems. 2016.</p>
              </pre>
            </div>
          </div>
        </section>
        <section id="signup" class="signup-section">
          <div class="container">
            <div class="row">
              <div class="col-md-10 col-lg-8 mx-auto text-center">
                <i class="far fa-paper-plane fa-2x mb-2 text-white"></i>
                <a class="btn btn-primary" href="#CFC" role="button">Call for contributions</a>
              </div>
            </div>
          </div>
        </section>
        <section class="contact-section bg-black">
          <div class="container">
            <div class="row">
              <div class="col-md-4 mb-3 mb-md-0">
                <div class="card py-4 h-100">
                  <div class="card-body text-center">
                    <i class="fas fa-map-marked-alt text-primary mb-2"></i>
                    <h4 class="text-uppercase m-0">Address</h4>
                    <hr class="my-4">
                    <div class="small text-black-50"> 1055 Canada Pl, Vancouver, BC V6C 0C3</div>
                  </div>
                </div>
              </div>
              <div class="col-md-4 mb-3 mb-md-0">
                <div class="card py-4 h-100">
                  <div class="card-body text-center">
                    <i class="fas fa-envelope text-primary mb-2"></i>
                    <h4 class="text-uppercase m-0">Email</h4>
                    <hr class="my-4">
                    <div class="small text-black-50">
                      <a href="#">sgoml.workshop@gmail.com</a>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-md-4 mb-3 mb-md-0">
                <div class="card py-4 h-100">
                  <div class="card-body text-center">
                    <i class="fas fa-monument text-primary mb-2"></i>
                    <h4 class="text-uppercase m-0">Vancouver Convention center</h4>
                    <hr class="my-4">
                    <div class="small text-black-50">Canada Place</div>
                  </div>
                </div>
              </div>
            </div>
            <div class="social d-flex justify-content-center">
              <a href="#" class="mx-2">
                <i class="fab fa-twitter"></i>
              </a>
              <a href="#" class="mx-2">
                <i class="fab fa-facebook-f"></i>
              </a>
              <a href="https://github.com/sgo-workshop" class="mx-2">
                <i class="fab fa-github"></i>
              </a>
            </div>
          </div>
        <!-- Footer -->
        <footer class="bg-black small text-center text-white-50">
          <div class="container">
            Copyright &copy; 2019 Smooth Games Optimization and Machine Learning Workshop
          </div>
        </footer>
        <!-- Bootstrap core JavaScript -->
        <script src="vendor/jquery/jquery.min.js"></script>
        <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
        <!-- Plugin JavaScript -->
        <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
        <!-- Custom scripts for this template -->
        <script src="js/grayscale.min.js"></script>
      </body>
    </html>